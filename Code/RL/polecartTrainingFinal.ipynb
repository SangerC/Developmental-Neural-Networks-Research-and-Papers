{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "polecartTrainingFinal.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imoh2T2tL8YZ",
        "outputId": "b2f42b3e-3a1f-4fee-8aba-bab4c00e7941"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0\tAverage length (last 100 episodes): 12.00\n",
            "Episode 50\tAverage length (last 100 episodes): 15.08\n",
            "Episode 100\tAverage length (last 100 episodes): 30.92\n",
            "Episode 150\tAverage length (last 100 episodes): 101.18\n",
            "Episode 200\tAverage length (last 100 episodes): 125.21\n",
            "Episode 250\tAverage length (last 100 episodes): 154.99\n",
            "Episode 300\tAverage length (last 100 episodes): 197.23\n",
            "Episode 350\tAverage length (last 100 episodes): 182.98\n",
            "Episode 400\tAverage length (last 100 episodes): 183.36\n",
            "Episode 450\tAverage length (last 100 episodes): 171.27\n",
            "Episode 500\tAverage length (last 100 episodes): 172.48\n",
            "Episode 550\tAverage length (last 100 episodes): 161.24\n",
            "Episode 600\tAverage length (last 100 episodes): 196.38\n",
            "Episode 650\tAverage length (last 100 episodes): 288.53\n",
            "Episode 700\tAverage length (last 100 episodes): 355.13\n",
            "Episode 750\tAverage length (last 100 episodes): 442.45\n",
            "Solved after 788 episodes! Running average is now 475.27. Last episode ran to 499 time steps.\n",
            "Episode 0\tAverage length (last 100 episodes): 16.00\n",
            "Episode 50\tAverage length (last 100 episodes): 39.76\n",
            "Episode 100\tAverage length (last 100 episodes): 43.06\n",
            "Episode 150\tAverage length (last 100 episodes): 46.85\n",
            "Episode 200\tAverage length (last 100 episodes): 36.17\n",
            "Episode 250\tAverage length (last 100 episodes): 25.97\n",
            "Episode 300\tAverage length (last 100 episodes): 31.72\n",
            "Episode 350\tAverage length (last 100 episodes): 31.34\n",
            "Episode 400\tAverage length (last 100 episodes): 18.08\n",
            "Episode 450\tAverage length (last 100 episodes): 8.99\n",
            "Episode 500\tAverage length (last 100 episodes): 8.42\n",
            "Episode 550\tAverage length (last 100 episodes): 8.30\n",
            "Episode 600\tAverage length (last 100 episodes): 8.21\n",
            "Episode 650\tAverage length (last 100 episodes): 8.23\n",
            "Episode 700\tAverage length (last 100 episodes): 8.42\n",
            "Episode 750\tAverage length (last 100 episodes): 8.39\n",
            "Episode 800\tAverage length (last 100 episodes): 8.19\n",
            "Episode 850\tAverage length (last 100 episodes): 8.18\n",
            "Episode 900\tAverage length (last 100 episodes): 8.28\n",
            "Episode 950\tAverage length (last 100 episodes): 8.29\n",
            "Episode 0\tAverage length (last 100 episodes): 21.00\n",
            "Episode 50\tAverage length (last 100 episodes): 22.39\n",
            "Episode 100\tAverage length (last 100 episodes): 26.88\n",
            "Episode 150\tAverage length (last 100 episodes): 29.88\n",
            "Episode 200\tAverage length (last 100 episodes): 39.95\n",
            "Episode 250\tAverage length (last 100 episodes): 111.75\n",
            "Episode 300\tAverage length (last 100 episodes): 328.56\n",
            "Solved after 341 episodes! Running average is now 476.24. Last episode ran to 452 time steps.\n",
            "Episode 0\tAverage length (last 100 episodes): 11.00\n",
            "Episode 50\tAverage length (last 100 episodes): 33.39\n",
            "Episode 100\tAverage length (last 100 episodes): 59.02\n",
            "Episode 150\tAverage length (last 100 episodes): 92.90\n",
            "Episode 200\tAverage length (last 100 episodes): 218.52\n",
            "Episode 250\tAverage length (last 100 episodes): 275.60\n",
            "Episode 300\tAverage length (last 100 episodes): 127.80\n",
            "Episode 350\tAverage length (last 100 episodes): 24.37\n",
            "Episode 400\tAverage length (last 100 episodes): 32.46\n",
            "Episode 450\tAverage length (last 100 episodes): 86.88\n",
            "Episode 500\tAverage length (last 100 episodes): 104.66\n",
            "Episode 550\tAverage length (last 100 episodes): 122.62\n",
            "Episode 600\tAverage length (last 100 episodes): 220.02\n",
            "Episode 650\tAverage length (last 100 episodes): 392.69\n",
            "Episode 700\tAverage length (last 100 episodes): 349.14\n",
            "Episode 750\tAverage length (last 100 episodes): 186.13\n",
            "Episode 800\tAverage length (last 100 episodes): 184.14\n",
            "Episode 850\tAverage length (last 100 episodes): 163.70\n",
            "Episode 900\tAverage length (last 100 episodes): 116.11\n",
            "Episode 950\tAverage length (last 100 episodes): 107.77\n",
            "Episode 0\tAverage length (last 100 episodes): 21.00\n",
            "Episode 50\tAverage length (last 100 episodes): 17.41\n",
            "Episode 100\tAverage length (last 100 episodes): 24.82\n",
            "Episode 150\tAverage length (last 100 episodes): 56.18\n",
            "Episode 200\tAverage length (last 100 episodes): 99.74\n",
            "Episode 250\tAverage length (last 100 episodes): 110.03\n",
            "Episode 300\tAverage length (last 100 episodes): 118.51\n",
            "Episode 350\tAverage length (last 100 episodes): 147.23\n",
            "Episode 400\tAverage length (last 100 episodes): 232.88\n",
            "Episode 450\tAverage length (last 100 episodes): 284.67\n",
            "Episode 500\tAverage length (last 100 episodes): 275.28\n",
            "Episode 550\tAverage length (last 100 episodes): 317.09\n",
            "Episode 600\tAverage length (last 100 episodes): 250.11\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "from torch.distributions import Categorical\n",
        "from torch.nn import parameter\n",
        "import time\n",
        "import sys\n",
        "\n",
        "\n",
        "\n",
        "class Policy(nn.Module):\n",
        "    def __init__(self, input, output, hidden):\n",
        "        super(Policy, self).__init__()\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "        self.layers.append(nn.Linear(input, hidden))\n",
        "        #self.layers.append(nn.Dropout(p=0.5))\n",
        "        self.layers.append(nn.ReLU())\n",
        "\n",
        "        self.layers.append(nn.Linear(hidden, hidden))\n",
        "        #self.layers.append(nn.Dropout(p=0.5))\n",
        "        self.layers.append(nn.ReLU())\n",
        "\n",
        "        self.layers.append(nn.Linear(hidden, output))\n",
        "        self.layers.append(nn.Softmax(dim=-1))\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "\n",
        "        # Overall reward and loss history\n",
        "        self.reward_history = []\n",
        "        self.loss_history = []\n",
        "        self.reset()\n",
        "\n",
        "    def addNode(self, layer, n = 1, optimizeAll = True):\n",
        "        actualLayer = layer * 2\n",
        "        nextLayer = actualLayer + 2\n",
        "\n",
        "        old = self.layers[actualLayer]\n",
        "        oldNextLayer = None\n",
        "        if nextLayer < len(self.layers):\n",
        "            oldNextLayer = self.layers[nextLayer]\n",
        "\n",
        "        new = nn.Linear(old.in_features, old.out_features + n)\n",
        "        if oldNextLayer:\n",
        "            newNext = nn.Linear(oldNextLayer.in_features + n, oldNextLayer.out_features)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for i in range(len(old.weight)):\n",
        "                for j in range(len(old.weight[i])):\n",
        "                    new.weight[i][j] = old.weight[i][j]\n",
        "            for i in range(len(old.weight), (len(new.weight))):\n",
        "                for j in range(len(new.weight[i])):\n",
        "                    new.weight[i][j] = 0\n",
        "            \n",
        "            if oldNextLayer:\n",
        "                for i in range(len(oldNextLayer.weight)):\n",
        "                    for j in range(len(oldNextLayer.weight[i])):\n",
        "                        newNext.weight[i][j] = oldNextLayer.weight[i][j]\n",
        "                    for j in range(len(oldNextLayer.weight[i]), len(newNext.weight[i])):\n",
        "                        newNext.weight[i][j] = 0\n",
        "        self.layers[actualLayer] = new\n",
        "        if oldNextLayer:\n",
        "            self.layers[nextLayer] = newNext\n",
        "\n",
        "    def addLayer(self, size):\n",
        "        for param in self.layers:\n",
        "          param.requires_grad_(False)\n",
        "        new_layer = nn.Linear(size, size)\n",
        "        torch.nn.init.constant_(new_layer.weight, 0)\n",
        "        new_layer.bias.data.fill_(0)\n",
        "        with torch.no_grad():\n",
        "            for i in range(len(new_layer.weight)):\n",
        "                new_layer.weight[i, i] = 1\n",
        "        self.layers.insert(len(self.layers) - 2, new_layer)\n",
        "        #self.layers.insert(len(self.layers) - 2, nn.Dropout(p=0.5))\n",
        "        self.layers.insert(len(self.layers) - 2, nn.ReLU())\n",
        "\n",
        "        #self.optimizer.add_param_group({\"params\" : [new_layer.weight]})\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        # Episode policy and reward history\n",
        "        self.episode_actions = torch.Tensor([])\n",
        "        self.episode_rewards = []\n",
        "\n",
        "    def forward(self, x, printThis = False):\n",
        "        self.last = copy.deepcopy(x)\n",
        "        for i in range(len(self.layers)):\n",
        "          x = self.layers[i](x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def predict(state, currEnv):\n",
        "    # Select an action (0 or 1) by running policy model\n",
        "    # and choosing based on the probabilities in state\n",
        "    new_state = [0]*input_size\n",
        "    x = 0\n",
        "    for i in range(len(env) - currEnv - 2):\n",
        "      x += env[i].observation_space.shape[0]\n",
        "\n",
        "    for i in range(env[currEnv].observation_space.shape[0]):\n",
        "      new_state[i + x] = state[i]\n",
        "    state = torch.from_numpy(np.array(new_state)).type(torch.FloatTensor)\n",
        "    action_probs = policy(state)\n",
        "    distribution = Categorical(action_probs)\n",
        "    action = distribution.sample()\n",
        "\n",
        "    # Add log probability of our chosen action to our history\n",
        "    policy.episode_actions = torch.cat([\n",
        "        policy.episode_actions,\n",
        "        distribution.log_prob(action).reshape(1)\n",
        "    ])\n",
        "\n",
        "    return action\n",
        "\n",
        "\n",
        "def update_policy():\n",
        "    R = 0\n",
        "    rewards = []\n",
        "\n",
        "    # Discount future rewards back to the present using gamma\n",
        "    for r in policy.episode_rewards[::-1]:\n",
        "        R = r + gamma * R\n",
        "        rewards.insert(0, R)\n",
        "\n",
        "    # Scale rewards\n",
        "    rewards = torch.FloatTensor(rewards)\n",
        "    rewards = (rewards - rewards.mean()) / \\\n",
        "        (rewards.std() + np.finfo(np.float32).eps)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = (torch.sum(torch.mul(policy.episode_actions, rewards).mul(-1), -1))\n",
        "\n",
        "    # Update network weights\n",
        "    policy.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    policy.optimizer.step()\n",
        "\n",
        "    # Save and intialize episode history counters\n",
        "    policy.loss_history.append(loss.item())\n",
        "    policy.reward_history.append(np.sum(policy.episode_rewards))\n",
        "    policy.reset()\n",
        "\n",
        "def train(episodes):\n",
        "    scores = []\n",
        "    currentEnv = 0\n",
        "    max_score = float('-inf')\n",
        "    max_increment = 0\n",
        "    layers_added = 0\n",
        "    for episode in range(episodes):\n",
        "        #if max_increment > 100 and layers_added < 1:\n",
        "          #policy.addLayer(hidden_size)\n",
        "          #print(\"added Layer\")\n",
        "         # layers_added += 1\n",
        "          #max_increment = 0\n",
        "\n",
        "\n",
        "        if episode == 250:\n",
        "          \n",
        "          #for param in policy.layers:\n",
        "          #  param.requires_grad_(False)\n",
        "          policy.addLayer(hidden_size)\n",
        "           # print(param)\n",
        "        \n",
        "        if episode == 500:\n",
        "          for param in policy.layers:\n",
        "            param.requires_grad_(True)\n",
        "          policy.optimizer = optim.Adam(policy.parameters(), lr=learning_rate)\n",
        "\n",
        "        state = env[currentEnv].reset()\n",
        "\n",
        "        for time in range(1000):\n",
        "            #state = state[:-1]\n",
        "            action = predict(state, currentEnv)\n",
        "\n",
        "            # Uncomment to render the visual state in a window\n",
        "            # env.render()\n",
        "\n",
        "            # Step through environment using chosen action\n",
        "            state, reward, done, _ = env[currentEnv].step(action.item())\n",
        "\n",
        "            # Save reward\n",
        "            policy.episode_rewards.append(reward)\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        update_policy()\n",
        "\n",
        "        # Calculate score to determine when the environment has been solved\n",
        "        scores.append(time)\n",
        "        mean_score = np.mean(scores[-100:])\n",
        "\n",
        "        if time > max_score:\n",
        "          max_score = time\n",
        "          max_increment = 0\n",
        "        else:\n",
        "          max_increment += 1\n",
        "\n",
        "        if episode % 50 == 0:\n",
        "            print('Episode {}\\tAverage length (last 100 episodes): {:.2f}'.format(episode, mean_score))\n",
        "            currentEnv += 1\n",
        "            if currentEnv >= len(env):\n",
        "              currentEnv = 0\n",
        "\n",
        "        if mean_score > env[currentEnv].spec.reward_threshold:\n",
        "            print(\"Solved after {} episodes! Running average is now {}. Last episode ran to {} time steps.\"\n",
        "                  .format(episode, mean_score, time))\n",
        "            for i in range(episode, episodes):\n",
        "               policy.reward_history.append(mean_score)\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "env = []\n",
        "env.append(gym.make('CartPole-v1'))\n",
        "#env.append(gym.make('Pendulum-v0'))\n",
        "\n",
        "folderName = 'perceptron'\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.01\n",
        "gamma = 0.99\n",
        "hidden_size = 16\n",
        "\n",
        "\n",
        "num_seeds = 10\n",
        "num_episodes = 1000\n",
        "\n",
        "\n",
        "input_size = sum(i.observation_space.shape[0] for i in env)\n",
        "#output_size = sum(i.action_space.n for i in env)\n",
        "output_size = env[0].action_space.n\n",
        "\n",
        "\n",
        "rewards_history_by_run = []\n",
        "time_by_run = []\n",
        "\n",
        "for i in range(num_seeds):\n",
        "\n",
        "  policy = Policy(input_size, output_size, hidden_size)\n",
        "  pytorchSeed = random.randint(0, 1000)\n",
        "  cartSeed = random.randint(0, 1000)\n",
        "  for i in env:\n",
        "\t  i.seed(cartSeed) \n",
        "  torch.manual_seed(pytorchSeed)\n",
        "  start = time.time()\n",
        "  train(episodes=num_episodes)\n",
        "  time_by_run.append(time.time() - start)\n",
        "  rewards_history_by_run.append(policy.reward_history)\n",
        "\n",
        "\t# number of episodes for rolling average\n",
        "\t# window = 50\n",
        "\n",
        "\t# fig, ((ax1), (ax2)) = plt.subplots(2, 1, sharey=True, figsize=[9, 9])\n",
        "\t# rolling_mean = pd.Series(policy.reward_history).rolling(window).mean()\n",
        "\t# std = pd.Series(policy.reward_history).rolling(window).std()\n",
        "\t# ax1.plot(rolling_mean)\n",
        "\t# ax1.fill_between(range(len(policy.reward_history)), rolling_mean -\n",
        "\t#                  std, rolling_mean+std, color='orange', alpha=0.2)\n",
        "\t# ax1.set_title(\n",
        "\t#     'Episode Length Moving Average ({}-episode window)'.format(window))\n",
        "\t# ax1.set_xlabel('Episode')\n",
        "\t# ax1.set_ylabel('Episode Length')\n",
        "\n",
        "\t# ax2.plot(policy.reward_history)\n",
        "\t# ax2.set_title('Episode Length')\n",
        "\t# ax2.set_xlabel('Episode')\n",
        "\t# ax2.set_ylabel('Episode Length')\n",
        "\n",
        "\t# fig.tight_layout(pad=2)\n",
        "\t#plt.savefig(folderName + '/PytorchSeed' + str(pytorchSeed) + 'cartSeed' + str(cartSeed) )\n",
        "\t\n",
        "\n",
        "average_history = []\n",
        "\n",
        "def get_avg(arr_of_arrs, i):\n",
        "    total = 0\n",
        "    for k in range(len(arr_of_arrs)):\n",
        "        total += arr_of_arrs[k][i]\n",
        "    return total/len(arr_of_arrs)\n",
        "\n",
        "for i in range(num_episodes):\n",
        "    average_history.append(get_avg(rewards_history_by_run, i))\n",
        "\n",
        "avg_time = sum(time_by_run)/len(time_by_run)\n",
        "with open('test', 'w') as f:\n",
        "    for item in average_history:\n",
        "        f.write(f\"{item}\\n\")\n",
        "    f.write(f\"Time: {avg_time}\")\n",
        "    \n",
        "## number of episodes for rolling average\n",
        "# window = 50\n",
        "\n",
        "# fig, ((ax1), (ax2)) = plt.subplots(2, 1, sharey=True, figsize=[9, 9])\n",
        "# rolling_mean = pd.Series(average_history).rolling(window).mean()\n",
        "# std = pd.Series(average_history).rolling(window).std()\n",
        "# ax1.plot(rolling_mean)\n",
        "# ax1.fill_between(range(len(average_history)), rolling_mean -\n",
        "#                  std, rolling_mean+std, color='orange', alpha=0.2)\n",
        "# ax1.set_title(\n",
        "#     'Episode Length Moving Average ({}-episode window)'.format(window))\n",
        "# ax1.set_xlabel('Episode')\n",
        "# ax1.set_ylabel('Episode Length')\n",
        "\n",
        "# ax2.plot(average_history)\n",
        "# ax2.set_title('Episode Length')\n",
        "# ax2.set_xlabel('Episode')\n",
        "# ax2.set_ylabel('Episode Length')\n",
        "\n",
        "# fig.tight_layout(pad=2)\n",
        "# plt.ylim(top=500)\n",
        "# #plt.savefig(folderName + '/average')\n",
        "# plt.show()"
      ]
    }
  ]
}