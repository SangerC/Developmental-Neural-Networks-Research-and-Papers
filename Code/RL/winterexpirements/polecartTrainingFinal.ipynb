{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "polecartTrainingFinal.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imoh2T2tL8YZ",
        "outputId": "acc6191a-a75c-43d8-d116-4a8a901ddd63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0\tAverage length (last 100 episodes): 12.00\n",
            "Episode 50\tAverage length (last 100 episodes): 19.88\n",
            "Episode 100\tAverage length (last 100 episodes): 23.89\n",
            "Episode 150\tAverage length (last 100 episodes): 34.96\n",
            "Episode 200\tAverage length (last 100 episodes): 43.69\n",
            "Episode 250\tAverage length (last 100 episodes): 41.51\n",
            "Episode 300\tAverage length (last 100 episodes): 40.42\n",
            "Episode 350\tAverage length (last 100 episodes): 37.30\n",
            "Episode 400\tAverage length (last 100 episodes): 37.37\n",
            "Episode 450\tAverage length (last 100 episodes): 47.99\n",
            "Episode 500\tAverage length (last 100 episodes): 56.35\n",
            "Episode 550\tAverage length (last 100 episodes): 61.64\n",
            "Episode 600\tAverage length (last 100 episodes): 63.69\n",
            "Episode 650\tAverage length (last 100 episodes): 59.17\n",
            "Episode 700\tAverage length (last 100 episodes): 61.47\n",
            "Episode 750\tAverage length (last 100 episodes): 56.51\n",
            "Episode 800\tAverage length (last 100 episodes): 47.51\n",
            "Episode 850\tAverage length (last 100 episodes): 71.08\n",
            "Episode 900\tAverage length (last 100 episodes): 113.60\n",
            "Episode 950\tAverage length (last 100 episodes): 208.25\n",
            "Episode 1000\tAverage length (last 100 episodes): 180.89\n",
            "Episode 1050\tAverage length (last 100 episodes): 156.98\n",
            "Episode 1100\tAverage length (last 100 episodes): 209.21\n",
            "Episode 1150\tAverage length (last 100 episodes): 219.09\n",
            "Episode 1200\tAverage length (last 100 episodes): 292.65\n",
            "Episode 1250\tAverage length (last 100 episodes): 271.24\n",
            "Episode 1300\tAverage length (last 100 episodes): 190.30\n",
            "Episode 1350\tAverage length (last 100 episodes): 133.94\n",
            "Episode 1400\tAverage length (last 100 episodes): 89.30\n",
            "Episode 1450\tAverage length (last 100 episodes): 57.93\n",
            "Episode 1500\tAverage length (last 100 episodes): 37.79\n",
            "Episode 1550\tAverage length (last 100 episodes): 51.50\n",
            "Episode 1600\tAverage length (last 100 episodes): 67.72\n",
            "Episode 1650\tAverage length (last 100 episodes): 66.71\n",
            "Episode 1700\tAverage length (last 100 episodes): 62.00\n",
            "Episode 1750\tAverage length (last 100 episodes): 70.71\n",
            "Episode 1800\tAverage length (last 100 episodes): 66.03\n",
            "Episode 1850\tAverage length (last 100 episodes): 101.67\n",
            "Episode 1900\tAverage length (last 100 episodes): 222.32\n",
            "Episode 1950\tAverage length (last 100 episodes): 243.63\n",
            "Episode 2000\tAverage length (last 100 episodes): 145.70\n",
            "Episode 2050\tAverage length (last 100 episodes): 167.53\n",
            "Episode 2100\tAverage length (last 100 episodes): 201.00\n",
            "Episode 2150\tAverage length (last 100 episodes): 177.04\n",
            "Episode 2200\tAverage length (last 100 episodes): 220.57\n",
            "Episode 2250\tAverage length (last 100 episodes): 253.36\n",
            "Episode 2300\tAverage length (last 100 episodes): 207.38\n",
            "Episode 2350\tAverage length (last 100 episodes): 200.79\n",
            "Episode 2400\tAverage length (last 100 episodes): 262.38\n",
            "Episode 2450\tAverage length (last 100 episodes): 264.38\n",
            "Episode 2500\tAverage length (last 100 episodes): 163.99\n",
            "Episode 2550\tAverage length (last 100 episodes): 164.74\n",
            "Episode 2600\tAverage length (last 100 episodes): 292.69\n",
            "Episode 2650\tAverage length (last 100 episodes): 266.26\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "from torch.distributions import Categorical\n",
        "from torch.nn import parameter\n",
        "import time\n",
        "import sys\n",
        "\n",
        "\n",
        "\n",
        "class Policy(nn.Module):\n",
        "    def __init__(self, input, output, hidden):\n",
        "        super(Policy, self).__init__()\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "        self.layers.append(nn.Linear(input, hidden))\n",
        "        #self.layers.append(nn.Dropout(p=0.5))\n",
        "        self.layers.append(nn.ReLU())\n",
        "\n",
        "        self.layers.append(nn.Linear(hidden, hidden))\n",
        "        self.layers.append(nn.ReLU())\n",
        "\n",
        "        self.layers.append(nn.Linear(hidden, output))\n",
        "        self.layers.append(nn.Softmax(dim=-1))\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "        # Overall reward and loss history\n",
        "        self.reward_history = []\n",
        "        self.loss_history = []\n",
        "        self.reset()\n",
        "\n",
        "    def addNode(self, layer, n = 1, optimizeAll = True):\n",
        "        actualLayer = layer * 2\n",
        "        nextLayer = actualLayer + 2\n",
        "\n",
        "        old = self.layers[actualLayer]\n",
        "        oldNextLayer = None\n",
        "        if nextLayer < len(self.layers):\n",
        "            oldNextLayer = self.layers[nextLayer]\n",
        "\n",
        "        new = nn.Linear(old.in_features, old.out_features + n)\n",
        "        if oldNextLayer:\n",
        "            newNext = nn.Linear(oldNextLayer.in_features + n, oldNextLayer.out_features)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for i in range(len(old.weight)):\n",
        "                for j in range(len(old.weight[i])):\n",
        "                    new.weight[i][j] = old.weight[i][j]\n",
        "            for i in range(len(old.weight), (len(new.weight))):\n",
        "                for j in range(len(new.weight[i])):\n",
        "                    new.weight[i][j] = 0\n",
        "            \n",
        "            if oldNextLayer:\n",
        "                for i in range(len(oldNextLayer.weight)):\n",
        "                    for j in range(len(oldNextLayer.weight[i])):\n",
        "                        newNext.weight[i][j] = oldNextLayer.weight[i][j]\n",
        "                    for j in range(len(oldNextLayer.weight[i]), len(newNext.weight[i])):\n",
        "                        newNext.weight[i][j] = 0\n",
        "        self.layers[actualLayer] = new\n",
        "        if oldNextLayer:\n",
        "            self.layers[nextLayer] = newNext\n",
        "\n",
        "    def addLayer(self, size):\n",
        "        for param in self.layers:\n",
        "          param.requires_grad_ = False\n",
        "        new_layer = nn.Linear(size, size)\n",
        "        torch.nn.init.constant_(new_layer.weight, 0)\n",
        "        new_layer.bias.data.fill_(0)\n",
        "        with torch.no_grad():\n",
        "            for i in range(len(new_layer.weight)):\n",
        "                new_layer.weight[i, i] = 1\n",
        "        self.layers.insert(len(self.layers) - 2, new_layer)\n",
        "        #self.layers.insert(len(self.layers) - 2, nn.Dropout(p=0.5))\n",
        "        self.layers.insert(len(self.layers) - 2, nn.ReLU())\n",
        "\n",
        "        self.optimizer.add_param_group({\"params\" : [new_layer.weight]})\n",
        "\n",
        "        #self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        # Episode policy and reward history\n",
        "        self.episode_actions = torch.Tensor([])\n",
        "        self.episode_rewards = []\n",
        "\n",
        "    def forward(self, x, printThis = False):\n",
        "        self.last = copy.deepcopy(x)\n",
        "        for i in range(len(self.layers)):\n",
        "          x = self.layers[i](x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def predict(state, currEnv):\n",
        "    # Select an action (0 or 1) by running policy model\n",
        "    # and choosing based on the probabilities in state\n",
        "    new_state = [0]*input_size\n",
        "    x = 0\n",
        "    for i in range(len(env) - currEnv - 2):\n",
        "      x += env[i].observation_space.shape[0]\n",
        "\n",
        "    for i in range(env[currEnv].observation_space.shape[0] - 1):\n",
        "      new_state[i + x] = state[i]\n",
        "    state = torch.from_numpy(np.array(new_state)).type(torch.FloatTensor)\n",
        "    action_probs = policy(state)\n",
        "    distribution = Categorical(action_probs)\n",
        "    action = distribution.sample()\n",
        "\n",
        "    # Add log probability of our chosen action to our history\n",
        "    policy.episode_actions = torch.cat([\n",
        "        policy.episode_actions,\n",
        "        distribution.log_prob(action).reshape(1)\n",
        "    ])\n",
        "\n",
        "    return action\n",
        "\n",
        "\n",
        "def update_policy():\n",
        "    R = 0\n",
        "    rewards = []\n",
        "\n",
        "    # Discount future rewards back to the present using gamma\n",
        "    for r in policy.episode_rewards[::-1]:\n",
        "        R = r + gamma * R\n",
        "        rewards.insert(0, R)\n",
        "\n",
        "    # Scale rewards\n",
        "    rewards = torch.FloatTensor(rewards)\n",
        "    rewards = (rewards - rewards.mean()) / \\\n",
        "        (rewards.std() + np.finfo(np.float32).eps)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = (torch.sum(torch.mul(policy.episode_actions, rewards).mul(-1), -1))\n",
        "\n",
        "    # Update network weights\n",
        "    policy.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    policy.optimizer.step()\n",
        "\n",
        "    # Save and intialize episode history counters\n",
        "    policy.loss_history.append(loss.item())\n",
        "    policy.reward_history.append(np.sum(policy.episode_rewards))\n",
        "    policy.reset()\n",
        "\n",
        "def train(episodes):\n",
        "    scores = []\n",
        "    currentEnv = 0\n",
        "    for episode in range(episodes):\n",
        "        #if episode == 250:\n",
        "         # policy.addLayer(hidden_size)\n",
        "          #for param in policy.layers:\n",
        "           # print(param)\n",
        "\n",
        "        state = env[currentEnv].reset()\n",
        "\n",
        "        for time in range(1000):\n",
        "            state = state[:-1]\n",
        "            action = predict(state, currentEnv)\n",
        "\n",
        "            # Uncomment to render the visual state in a window\n",
        "            # env.render()\n",
        "\n",
        "            # Step through environment using chosen action\n",
        "            state, reward, done, _ = env[currentEnv].step(action.item())\n",
        "\n",
        "            # Save reward\n",
        "            policy.episode_rewards.append(reward)\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        update_policy()\n",
        "\n",
        "        # Calculate score to determine when the environment has been solved\n",
        "        scores.append(time)\n",
        "        mean_score = np.mean(scores[-100:])\n",
        "\n",
        "        if episode % 50 == 0:\n",
        "            print('Episode {}\\tAverage length (last 100 episodes): {:.2f}'.format(episode, mean_score))\n",
        "            currentEnv += 1\n",
        "            if currentEnv >= len(env):\n",
        "              currentEnv = 0\n",
        "\n",
        "        if mean_score > env[currentEnv].spec.reward_threshold:\n",
        "            print(\"Solved after {} episodes! Running average is now {}. Last episode ran to {} time steps.\"\n",
        "                  .format(episode, mean_score, time))\n",
        "            for i in range(episode, episodes):\n",
        "               policy.reward_history.append(mean_score)\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "env = []\n",
        "env.append(gym.make('CartPole-v1'))\n",
        "#env.append(gym.make('Pendulum-v0'))\n",
        "\n",
        "folderName = 'perceptron'\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.01\n",
        "gamma = 0.99\n",
        "hidden_size = 16\n",
        "\n",
        "\n",
        "num_seeds = 10\n",
        "num_episodes = 3000\n",
        "\n",
        "\n",
        "input_size = sum(i.observation_space.shape[0] for i in env) - 1\n",
        "#output_size = sum(i.action_space.n for i in env)\n",
        "output_size = env[0].action_space.n\n",
        "\n",
        "\n",
        "rewards_history_by_run = []\n",
        "time_by_run = []\n",
        "\n",
        "for i in range(num_seeds):\n",
        "\n",
        "  policy = Policy(input_size, output_size, hidden_size)\n",
        "  pytorchSeed = random.randint(0, 1000)\n",
        "  cartSeed = random.randint(0, 1000)\n",
        "  for i in env:\n",
        "\t  i.seed(cartSeed) \n",
        "  torch.manual_seed(pytorchSeed)\n",
        "  start = time.time()\n",
        "  train(episodes=num_episodes)\n",
        "  time_by_run.append(time.time() - start)\n",
        "  rewards_history_by_run.append(policy.reward_history)\n",
        "\n",
        "\t# number of episodes for rolling average\n",
        "\t# window = 50\n",
        "\n",
        "\t# fig, ((ax1), (ax2)) = plt.subplots(2, 1, sharey=True, figsize=[9, 9])\n",
        "\t# rolling_mean = pd.Series(policy.reward_history).rolling(window).mean()\n",
        "\t# std = pd.Series(policy.reward_history).rolling(window).std()\n",
        "\t# ax1.plot(rolling_mean)\n",
        "\t# ax1.fill_between(range(len(policy.reward_history)), rolling_mean -\n",
        "\t#                  std, rolling_mean+std, color='orange', alpha=0.2)\n",
        "\t# ax1.set_title(\n",
        "\t#     'Episode Length Moving Average ({}-episode window)'.format(window))\n",
        "\t# ax1.set_xlabel('Episode')\n",
        "\t# ax1.set_ylabel('Episode Length')\n",
        "\n",
        "\t# ax2.plot(policy.reward_history)\n",
        "\t# ax2.set_title('Episode Length')\n",
        "\t# ax2.set_xlabel('Episode')\n",
        "\t# ax2.set_ylabel('Episode Length')\n",
        "\n",
        "\t# fig.tight_layout(pad=2)\n",
        "\t#plt.savefig(folderName + '/PytorchSeed' + str(pytorchSeed) + 'cartSeed' + str(cartSeed) )\n",
        "\t\n",
        "\n",
        "average_history = []\n",
        "\n",
        "def get_avg(arr_of_arrs, i):\n",
        "    total = 0\n",
        "    for k in range(len(arr_of_arrs)):\n",
        "        total += arr_of_arrs[k][i]\n",
        "    return total/len(arr_of_arrs)\n",
        "\n",
        "for i in range(num_episodes):\n",
        "    average_history.append(get_avg(rewards_history_by_run, i))\n",
        "\n",
        "avg_time = sum(time_by_run)/len(time_by_run)\n",
        "with open('twolayer', 'w') as f:\n",
        "    for item in average_history:\n",
        "        f.write(f\"{item}\\n\")\n",
        "    f.write(f\"Time: {avg_time}\")\n",
        "    \n",
        "## number of episodes for rolling average\n",
        "# window = 50\n",
        "\n",
        "# fig, ((ax1), (ax2)) = plt.subplots(2, 1, sharey=True, figsize=[9, 9])\n",
        "# rolling_mean = pd.Series(average_history).rolling(window).mean()\n",
        "# std = pd.Series(average_history).rolling(window).std()\n",
        "# ax1.plot(rolling_mean)\n",
        "# ax1.fill_between(range(len(average_history)), rolling_mean -\n",
        "#                  std, rolling_mean+std, color='orange', alpha=0.2)\n",
        "# ax1.set_title(\n",
        "#     'Episode Length Moving Average ({}-episode window)'.format(window))\n",
        "# ax1.set_xlabel('Episode')\n",
        "# ax1.set_ylabel('Episode Length')\n",
        "\n",
        "# ax2.plot(average_history)\n",
        "# ax2.set_title('Episode Length')\n",
        "# ax2.set_xlabel('Episode')\n",
        "# ax2.set_ylabel('Episode Length')\n",
        "\n",
        "# fig.tight_layout(pad=2)\n",
        "# plt.ylim(top=500)\n",
        "# #plt.savefig(folderName + '/average')\n",
        "# plt.show()"
      ]
    }
  ]
}